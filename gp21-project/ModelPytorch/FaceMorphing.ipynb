{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d358680",
   "metadata": {},
   "source": [
    "This jupyter notebook contains the code for face morphing using a trained Convolutional Mesh Autoencoder. The architecture is based on the paper [Generating 3D faces using Convolutional Mesh\n",
    "Autoencoders](https://coma.is.tue.mpg.de/) and is trained on 44 neutral example faces. As template the mesh *headtemplate_noneck_lesshead_4k.obj* was used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39aab5c",
   "metadata": {},
   "source": [
    "![Face morphing with Autoencoder](../results/faceModel/example.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565484d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igl\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "%matplotlib notebook\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from model import Decoder, Encoder, Downscale\n",
    "import torch\n",
    "from torch_geometric.io import write_off, read_obj\n",
    "from torch_geometric.transforms import FaceToEdge\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "else:\n",
    "        dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "addEdges = FaceToEdge(remove_faces=False)\n",
    "\n",
    "# Downsampled versions of the headtemplate_noneck_lesshead_4k.obj \n",
    "mesh0 = read_obj(\"./recon_meshes/mesh0.obj\")\n",
    "mesh1 = read_obj(\"./recon_meshes/mesh1.obj\")\n",
    "mesh2 = read_obj(\"./recon_meshes/mesh2.obj\")\n",
    "mesh3 = read_obj(\"./recon_meshes/mesh3.obj\")\n",
    "mesh4 = read_obj(\"./recon_meshes/mesh4.obj\")\n",
    "v1 = genfromtxt(\"./recon_meshes/V1.txt\")\n",
    "v2 = genfromtxt(\"./recon_meshes/V2.txt\")\n",
    "v3 = genfromtxt(\"./recon_meshes/V3.txt\")\n",
    "v4 = genfromtxt(\"./recon_meshes/V4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ebf70",
   "metadata": {},
   "source": [
    "The output reconstruction of the faces using the trained model is a bit noisy therefore the output is smoothed after reconstruction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04ca73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(v, f):\n",
    "    l = igl.cotmatrix(v, f)\n",
    "    n = igl.per_vertex_normals(v, f)*0.5+0.5\n",
    "\n",
    "\n",
    "    m = igl.massmatrix(v, f, igl.MASSMATRIX_TYPE_BARYCENTRIC)\n",
    "    s = (m - 10*l)\n",
    "    b = m.dot(v)\n",
    "    v = spsolve(s, m.dot(v))\n",
    "    vs=v\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48dd29",
   "metadata": {},
   "source": [
    "Define decoder and encoder checkpoints that should be loaded. There are two trained models at the moment *decoder_44faces.pth/encoder_44faces.pth* and *decoder_44faces_lr5e-4.pth/encoder_44faces_lr5e-4.pth*. Both work well while the second one was trained for a longer time with a smaller learning rate wich improved the results slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_checkpoint = \"ckt/decoder_44faces_lr5e-4.pth\"\n",
    "encoder_checkpoint = \"ckt/encoder_44faces_lr5e-4.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c324185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(code=[0.,0.,0.,0.,0.,0.,0.,0.]):\n",
    "\n",
    "    tensor_code = torch.tensor(code).to(device)\n",
    "\n",
    "    downscaler = Downscale(mesh0, mesh1, mesh2, mesh3, mesh4, v1, v2, v3, v4, device)\n",
    "    decoder = Decoder(downscaler).to(device)\n",
    "    decoder.load_state_dict(torch.load(decoder_checkpoint))\n",
    "\n",
    "    reconstruction = decoder(tensor_code)\n",
    "    vertices = reconstruction.detach().cpu().numpy()\n",
    "\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc5423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(pathtomesh):\n",
    "    \n",
    "    target = read_obj(pathtomesh).to(device)\n",
    "    target = addEdges(target)\n",
    "    target.x = target.pos\n",
    "    \n",
    "    downscaler = Downscale(mesh0, mesh1, mesh2, mesh3, mesh4, v1, v2, v3, v4, device)\n",
    "    encoder = Encoder(downscaler).to(device)\n",
    "    encoder.load_state_dict(torch.load(encoder_checkpoint))\n",
    "\n",
    "    code = encoder(target)\n",
    "    \n",
    "    return code.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab997a66",
   "metadata": {},
   "source": [
    "The following code encodes the the template to get a vector of size 8 representing the neutral face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e717f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02449566 -0.08304243  1.4615785   0.13484523  0.4690101  -0.30835578\n",
      "  0.14531352  0.17747006]\n"
     ]
    }
   ],
   "source": [
    "code = encode(\"./recon_meshes/mesh0.obj\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa6e78",
   "metadata": {},
   "source": [
    "Now the parameters can be edited to change the face interactively in the meshplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9916728",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa99a8430f6c4a9389e7b706ed21995e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.1754722…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ee5afab38e4e2b9bf5a6c31301c7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='param0', max=2.0, min=-2.0), FloatSlider(value=0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v, f = igl.read_triangle_mesh(\"./recon_meshes/mesh0.obj\")\n",
    "v=smooth(decode(code), f)\n",
    "p = mp.plot(v,f)\n",
    "\n",
    "@interact(param0=(-2., 2.), param1=(-2., 2.), param2=(-2., 2.), param3=(-2., 2.), param4=(-2., 2.), param5=(-2., 2.), param6=(-2., 2.), param7=(-2., 2.))\n",
    "def change(param0=(-2., 2.), param1=(-2., 2.), param2=(-2., 2.), param3=(-2., 2.), param4=(-2., 2.), param5=(-2., 2.), param6=(-2., 2.), param7=(-2., 2.)):\n",
    "    res = code.copy()\n",
    "    res += [param0, param1, param2, param3, param4, param5, param6 ,param7]\n",
    "    p.update_object(vertices=smooth(decode(res),f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5563839",
   "metadata": {},
   "source": [
    "In the next part two faces can be loaded and then morphed between them by interpolating their representative code of 8 numbers. (*In this case two new faces were loaded to see if the model generalizes well, but it doesn't know what to do with them and generates nearly the same as the template, so training on only 44 faces probably isn't enough*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb28f221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1208967ab06541a2b00ae802758f01ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0867919…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36065877f0514de092fa47155b08e099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='morph', max=1.0), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "facecode1 = encode(\"../data/faces_warped/netural_henry.obj\")\n",
    "facecode2 = encode(\"../data/faces_warped/neutral_julian.obj\")\n",
    "\n",
    "v = decode(facecode1)\n",
    "vs = smooth(v,f)\n",
    "p1 = mp.plot(vs,f)\n",
    "\n",
    "@interact(morph = (0.))\n",
    "def change(morph = (0., 1.)):\n",
    "    res = facecode1 * (1-morph) + facecode2 * morph\n",
    "    vs = smooth(decode(res), f)\n",
    "    p1.update_object(vertices=vs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceModel",
   "language": "python",
   "name": "facemodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
